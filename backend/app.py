import os
import asyncio
from typing import Dict, List
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv
from fastapi import FastAPI, UploadFile, File, HTTPException, Form
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from openai import OpenAI

# ---- æ™ºèƒ½æ¨¡å‹é™çº§ç³»ç»Ÿ ----
from model_fallback import ModelFallbackManager, ModelTier, MODEL_CONFIGS, ModelConfig, estimate_tokens

# ---- ç¯å¢ƒå˜é‡ ----
load_dotenv()
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
# Initialize OpenAI client with graceful handling of missing API key
openai_api_key = os.getenv("OPENAI_API_KEY")
if openai_api_key:
    client = OpenAI(api_key=openai_api_key)
    print("âœ… OpenAI client initialized successfully")
else:
    client = None
    print("âš ï¸ No OpenAI API key found - OpenAI models will be unavailable")

# Initialize model fallback manager
model_manager = ModelFallbackManager(client)

# ---- RAG ç»„ä»¶ï¼ˆPinecone + LangChainï¼‰----
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone

USE_RAG = True
INDEX_NAME = os.environ.get("PINECONE_INDEX_NAME", "default-index")
EMBEDDING_MODEL = os.environ.get("EMBEDDING_MODEL", "text-embedding-3-small")

# å»¶è¿Ÿåˆå§‹åŒ–å‘é‡å­˜å‚¨å’ŒåµŒå…¥æ¨¡å‹ï¼Œé¿å…å¯åŠ¨æ—¶è¿æ¥å¤±è´¥
embeddings = None
vectorstore = None
retriever = None

def init_vectorstore():
    global retriever
    try:
        # Initialize Pinecone client
        from pinecone import Pinecone
        pc = Pinecone(api_key=os.environ.get("PINECONE_API_KEY"))
        pinecone_index = pc.Index(INDEX_NAME)
        
        # Initialize embeddings
        from langchain_huggingface import HuggingFaceEmbeddings
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        
        vectorstore = PineconeVectorStore(
            index=pinecone_index, 
            embedding=embeddings
        )
        retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
        print("âœ… Vectorstore initialized successfully")
        return True
    except Exception as e:
        print(f"âš ï¸ Failed to initialize vectorstore: {e}")
        print("ğŸ”§ RAG functionality will be disabled, but chat will continue to work")
        return False

# ---- è®°å¿†å’Œå­¦ä¹ ç³»ç»Ÿ ----
from memory_system import ConversationMemory, AdaptivePersonality

# ---- FastAPI ä¸å†…å­˜ä¼šè¯ ----
app = FastAPI(title="RAG Chat Backend", version="1.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True,
    allow_methods=["*"], allow_headers=["*"]
)

@app.on_event("startup")
async def startup_event():
    print("ğŸš€ Starting RAG Chat Backend...")
    init_vectorstore()
    print("âœ… Backend startup complete")

SESSIONS: Dict[str, List[Dict[str, str]]] = {}

# åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ
memory_system = ConversationMemory("conversations.db")
personality_system = AdaptivePersonality(memory_system)

class ChatRequest(BaseModel):
    session_id: str
    message: str
    max_history: int | None = 8

class ChatResponse(BaseModel):
    answer: str

class StreamChatRequest(ChatRequest):
    stream: bool | None = True

def _sse_format(event: str | None, data: str):
    if event:
        return f"event: {event}\ndata: {data}\n\n"
    return f"data: {data}\n\n"

@app.get("/health")
def health():
    return {"status": "ok"}

def retrieve_context(query: str, k: int = 4):
    try:
        if not USE_RAG or retriever is None:
            print(f"ğŸ” RAG disabled or retriever not available, using no context")
            docs = []
        else:
            docs = retriever.invoke(query)
            print(f"ğŸ” Retrieved {len(docs)} documents for query: '{query}'")
    except Exception as e:
        print(f"âš ï¸ RAG retrieval failed (falling back to no context): {e}")
        docs = []
    
    cites, parts = [], []
    for i, d in enumerate(docs, 1):
        src  = d.metadata.get("source", "unknown")
        page = d.metadata.get("page")
        preview = d.page_content[:240].replace("\n", " ")
        cites.append({"source": src, "preview": preview, "page": str(page) if page is not None else ""})
        parts.append(f"[{i}] {d.page_content}\n(source: {src}" + (f", page: {page})" if page is not None else ")"))
    
    context = "\n\n".join(parts)
    print(f"ğŸ“„ Context length: {len(context)} chars")
    return context, cites

def build_messages(session_id: str, user_msg: str, system_prompt: str, max_history: int):
    history = SESSIONS.get(session_id, [])
    hist = history[-(max_history*2):] if (max_history and max_history>0) else history
    return [{"role":"system","content":system_prompt}] + hist + [{"role":"user","content":user_msg}]

@app.post("/api/chat", response_model=ChatResponse)
async def chat(req: ChatRequest):
    # 1) æ£€ç´¢ä¸Šä¸‹æ–‡
    context, _ = retrieve_context(req.message, k=4)  # å¿½ç•¥citations
    
    if context and context.strip():
        # æœ‰ç›¸å…³çŸ¥è¯†åº“å†…å®¹ï¼Œä½¿ç”¨RAGæ¨¡å¼
        instruction = (
            "You are a helpful AI assistant. Use the Retrieved Context to answer the question when relevant. "
            "If the context contains relevant information, prioritize it in your answer. "
            "If the context is not relevant or helpful, you can also use your general knowledge to provide a helpful response."
        )
        system_prompt = f"{instruction}\n\nRetrieved Context:\n{context}"
    else:
        # æ²¡æœ‰ç›¸å…³çŸ¥è¯†åº“å†…å®¹ï¼Œä½¿ç”¨æ™®é€šå¯¹è¯æ¨¡å¼
        instruction = (
            "You are a helpful AI assistant. Answer questions using your knowledge and capabilities. "
            "Be conversational, helpful, and informative."
        )
        system_prompt = instruction

    # 2) æ„é€ æ¶ˆæ¯
    messages = build_messages(
        session_id=req.session_id, user_msg=req.message,
        system_prompt=system_prompt, max_history=req.max_history or 8
    )
    
    # 3) æ™ºèƒ½æ¨¡å‹é€‰æ‹©å’Œæ‰§è¡ŒèŠå¤© (with automatic fallback)
    estimated_tokens = estimate_tokens(str(messages))
    selected_tier, selection_reason = await model_manager.get_optimal_model(estimated_tokens)
    
    answer = None
    model_info = None
    max_fallback_attempts = 4  # Try up to 4 different models (all OpenAI + local)
    
    for attempt in range(max_fallback_attempts):
        try:
            print(f"ğŸ”„ Attempt {attempt + 1}: Trying {selected_tier.value}")
            # 4) æ‰§è¡ŒèŠå¤©
            completion = await model_manager.execute_chat(messages, selected_tier, stream=False)
            print(f"âœ… Got completion from {selected_tier.value}")
            
            if selected_tier in [ModelTier.GPT4, ModelTier.GPT4_MINI, ModelTier.GPT35]:
                answer = completion.choices[0].message.content
            else:
                # Handle local model response (Ollama)
                try:
                    if hasattr(completion, 'json'):
                        # Ollama response
                        response_data = completion.json()
                        answer = response_data.get('response', 'Local model response')
                        print(f"âœ… Local model response: {answer[:100]}...")
                    elif hasattr(completion, 'text'):
                        answer = completion.text
                        print(f"âœ… Local model text: {answer[:100]}...")
                    else:
                        answer = str(completion)
                        print(f"âœ… Local model string: {answer[:100]}...")
                except Exception as parse_error:
                    print(f"âš ï¸ Error parsing local model response: {parse_error}")
                    answer = "Local model responded but couldn't parse the response."
            
            # Success! Break out of retry loop
            model_info = model_manager.get_current_model_info()
            break
            
        except Exception as e:
            print(f"Model {selected_tier.value} failed (attempt {attempt + 1}): {str(e)}")
            
            # Check if this is a quota/rate limit error OR connection error
            error_str = str(e)
            is_recoverable_error = (
                "429" in error_str or 
                "insufficient_quota" in error_str or 
                "rate_limit" in error_str or
                "Connection error" in error_str or
                "timeout" in error_str.lower()
            )
            
            if is_recoverable_error:
                # Try to get next available model
                print(f"ğŸ”„ Recoverable error detected, attempting fallback...")
                try:
                    next_tier, next_reason = await model_manager.get_optimal_model(estimated_tokens)
                    if next_tier != selected_tier:
                        print(f"ğŸ”„ Switching from {selected_tier.value} to {next_tier.value}")
                        selected_tier = next_tier
                        selection_reason = next_reason
                        continue  # Try again with new model
                except Exception as fallback_error:
                    print(f"Fallback selection failed: {fallback_error}")
            
            # If this is the last attempt or not a recoverable error, fail
            if attempt == max_fallback_attempts - 1:
                answer = f"âš ï¸ All models temporarily unavailable. Error: {str(e)}"
                model_info = {"model_name": "fallback", "tier": "error"}
                break

    # ğŸ§  é›†æˆè®°å¿†å’Œä¸ªæ€§åŒ–ç³»ç»Ÿ (only if we got a successful response)
    if answer and not answer.startswith("âš ï¸"):
        try:
            user_style = personality_system.analyze_user_style(req.session_id)
            relevant_memories = memory_system.get_relevant_memory(req.message, req.session_id, limit=3)
            
            # å¦‚æœæœ‰ç›¸å…³è®°å¿†ï¼Œå¢å¼ºå›ç­”
            if relevant_memories:
                memory_context = "\n\nğŸ§  **Based on our previous conversations:**\n"
                for mem in relevant_memories[:2]:  # åªæ˜¾ç¤ºæœ€ç›¸å…³çš„2ä¸ª
                    memory_context += f"- {mem['user_message'][:50]}... â†’ {mem['assistant_response'][:100]}...\n"
                answer = memory_context + answer
            
            # æ ¹æ®ç”¨æˆ·é£æ ¼è°ƒæ•´å›åº”
            answer = personality_system.adapt_response_style(answer, user_style)
            
            # Add model info with learning stats
            learning_stats = memory_system.get_learning_stats()
            answer += f"\n\n*[Used: {model_info['model_name']} - {selection_reason}]*"
            if learning_stats['total_conversations'] > 0:
                answer += f"\n*[ğŸ’¡ ä» {learning_stats['total_conversations']} æ¬¡å¯¹è¯ä¸­å­¦ä¹ ï¼ŒæŒæ¡ {learning_stats['learned_concepts']} ä¸ªæ¦‚å¿µ]*"
        except Exception as memory_error:
            print(f"Memory/personality error (non-critical): {memory_error}")
    
    # Set default answer if nothing was set
    if not answer:
        answer = "âš ï¸ Unable to generate response. Please try again."

    # ğŸ¯ å­˜å‚¨åˆ°è®°å¿†ç³»ç»Ÿ
    try:
        memory_system.store_conversation(
            session_id=req.session_id,
            user_message=req.message,
            assistant_response=answer,
            context_used=context if context else None
        )
    except Exception as memory_error:
        print(f"Memory storage error: {memory_error}")

    # 5) å­˜ä¼šè¯ (ä¿æŒåŸæœ‰åŠŸèƒ½)
    SESSIONS.setdefault(req.session_id, []).extend([
        {"role":"user","content":req.message},
        {"role":"assistant","content":answer}
    ])
    return ChatResponse(answer=answer)

@app.post("/api/chat_stream")
async def chat_stream(req: StreamChatRequest):
    # Build context
    context, _ = retrieve_context(req.message, k=4)  # å¿½ç•¥citations
    
    if context and context.strip():
        # æœ‰ç›¸å…³çŸ¥è¯†åº“å†…å®¹ï¼Œä½¿ç”¨RAGæ¨¡å¼
        instruction = (
            "You are a helpful AI assistant. Use the Retrieved Context to answer the question when relevant. "
            "If the context contains relevant information, prioritize it in your answer. "
            "If the context is not relevant or helpful, you can also use your general knowledge to provide a helpful response."
        )
        system_prompt = f"{instruction}\n\nRetrieved Context:\n{context}"
    else:
        # æ²¡æœ‰ç›¸å…³çŸ¥è¯†åº“å†…å®¹ï¼Œä½¿ç”¨æ™®é€šå¯¹è¯æ¨¡å¼
        instruction = (
            "You are a helpful AI assistant. Answer questions using your knowledge and capabilities. "
            "Be conversational, helpful, and informative."
        )
        system_prompt = instruction
    messages = build_messages(req.session_id, req.message, system_prompt, req.max_history or 8)

    async def token_stream():
        accumulated = []
        import json
        
        # æ™ºèƒ½æ¨¡å‹é€‰æ‹©
        estimated_tokens = estimate_tokens(str(messages))
        selected_tier, selection_reason = await model_manager.get_optimal_model(estimated_tokens)
        
        # Send model info
        model_info = model_manager.get_current_model_info()
        yield _sse_format("model_info", json.dumps({
            "model": model_info['model_name'],
            "tier": model_info['tier'],
            "reason": selection_reason
        }))
        
        # Try multiple models with automatic fallback
        max_retries = 3
        current_tier = selected_tier
        success = False
        
        for retry_count in range(max_retries):
            try:
                # Execute streaming chat
                stream = await model_manager.execute_chat(messages, current_tier, stream=True)
                
                if current_tier in [ModelTier.GPT4, ModelTier.GPT4_MINI, ModelTier.GPT35]:
                    # Handle OpenAI streaming
                    for chunk in stream:
                        choice = chunk.choices[0]
                        delta = getattr(choice, 'delta', None) or getattr(choice, 'message', None) or None
                        if delta and delta.content:
                            text_part = delta.content
                            accumulated.append(text_part)
                            yield _sse_format(None, text_part)
                else:
                    # Handle local model streaming (Ollama format)
                    import json as json_lib
                    for line in stream.iter_lines():
                        if line:
                            try:
                                chunk_data = json_lib.loads(line)
                                if 'response' in chunk_data:
                                    text_part = chunk_data['response']
                                    accumulated.append(text_part)
                                    yield _sse_format(None, text_part)
                            except:
                                continue
                
                success = True
                break  # Success, exit retry loop
                            
            except Exception as e:
                error_msg = f"âš ï¸ Model {model_info.get('model_name', 'unknown')} failed: {str(e)}"
                print(f"Model failure attempt {retry_count + 1}: {error_msg}")
                
                # Check if this is a rate limit error and try next model
                if "rate_limit_exceeded" in str(e) or "429" in str(e) or "insufficient_quota" in str(e):
                    # Try to get next available model
                    try:
                        next_tier, next_reason = await model_manager.get_optimal_model(estimated_tokens)
                        if next_tier != current_tier and retry_count < max_retries - 1:
                            print(f"ğŸ”„ Rate limit hit, switching from {current_tier.value} to {next_tier.value}")
                            current_tier = next_tier
                            
                            # Update model info for user
                            model_info = model_manager.get_current_model_info()
                            yield _sse_format("model_info", json.dumps({
                                "model": model_info['model_name'],
                                "tier": next_tier.value,
                                "reason": f"Fallback: {next_reason}"
                            }))
                            continue  # Try again with new model
                    except:
                        pass
                
                # If this is the last retry AND it's not a quota/rate limit error, show error
                # Don't show quota errors to user - they should be handled silently by fallback
                if retry_count == max_retries - 1 and not ("429" in str(e) or "insufficient_quota" in str(e) or "rate_limit_exceeded" in str(e)):
                    accumulated.append(error_msg)
                    yield _sse_format(None, error_msg)
                    break
                elif retry_count == max_retries - 1:
                    # For quota errors on final retry, just break without showing error
                    print(f"All models exhausted due to quota limits")
                    break
        
        full_text = ''.join(accumulated)
        
        # ğŸ§  ä»è®°å¿†ä¸­è·å–ç›¸å…³ä¸Šä¸‹æ–‡å¹¶å­¦ä¹ ç”¨æˆ·é£æ ¼
        user_style = personality_system.analyze_user_style(req.session_id)
        relevant_memories = memory_system.get_relevant_memory(req.message, req.session_id, limit=3)
        
        # æ ¹æ®ç”¨æˆ·é£æ ¼è°ƒæ•´å›åº”
        adapted_response = personality_system.adapt_response_style(full_text, user_style)
        
        # Add model signature with learning info
        learning_stats = memory_system.get_learning_stats()
        signature = f"\n\n*[Generated by: {model_info.get('model_name', 'unknown')} - {selection_reason}]*"
        if learning_stats['total_conversations'] > 0:
            signature += f"\n*[ğŸ’¡ ä» {learning_stats['total_conversations']} æ¬¡å¯¹è¯ä¸­å­¦ä¹ ï¼ŒæŒæ¡ {learning_stats['learned_concepts']} ä¸ªæ¦‚å¿µ]*"
        
        adapted_response += signature
        yield _sse_format(None, signature)
        
        # ğŸ¯ å­˜å‚¨å¯¹è¯åˆ°è®°å¿†ç³»ç»Ÿ (å¼‚æ­¥å­¦ä¹ )
        try:
            conversation_id = memory_system.store_conversation(
                session_id=req.session_id,
                user_message=req.message,
                assistant_response=adapted_response,
                context_used=context if context else None,
                topics=None,  # å¯ä»¥é€šè¿‡NLPæå–
                sentiment=0.5  # å¯ä»¥é€šè¿‡æƒ…æ„Ÿåˆ†æå¾—å‡º
            )
        except Exception as memory_error:
            print(f"Memory storage error: {memory_error}")
        
        # persist session (ä¿æŒåŸæœ‰åŠŸèƒ½)
        SESSIONS.setdefault(req.session_id, []).extend([
            {"role":"user","content":req.message},
            {"role":"assistant","content":adapted_response}
        ])
        yield _sse_format("done", "true")

    return StreamingResponse(token_stream(), media_type="text/event-stream")

@app.get("/api/model_status")
async def get_model_status():
    """Get current model status and availability"""
    model_info = model_manager.get_current_model_info()
    
    # Check availability of all models
    model_availability = {}
    for tier in ModelTier:
        try:
            can_use = await model_manager._can_use_model(tier, 1000)  # Test with 1k tokens
            model_availability[tier.value] = {
                "available": can_use,
                "config": {
                    "name": MODEL_CONFIGS[tier].name,
                    "max_tokens": MODEL_CONFIGS[tier].max_tokens,
                    "cost_per_1k": MODEL_CONFIGS[tier].cost_per_1k_tokens,
                    "requires_api": MODEL_CONFIGS[tier].requires_api
                }
            }
        except:
            model_availability[tier.value] = {"available": False, "config": None}
    
    return {
        "current_model": model_info,
        "model_availability": model_availability,
        "quota_cache": model_manager.quota_cache,
        "failure_counts": {k.value: v for k, v in model_manager.failure_counts.items()}
    }

def _get_model_display_name(tier: ModelTier, config: ModelConfig) -> str:
    """Get user-friendly display name for model"""
    display_names = {
        ModelTier.GPT4: "GPT-4o (Best Quality)",
        ModelTier.GPT4_MINI: "GPT-4o Mini (Balanced)",
        ModelTier.GPT35: "GPT-3.5 Turbo (Fast)",
        ModelTier.LOCAL: "Llama 3.1 8B (Local)"
    }
    return display_names.get(tier, config.name)

def _get_model_description(tier: ModelTier, config: ModelConfig) -> str:
    """Get description for model"""
    descriptions = {
        ModelTier.GPT4: "Highest quality responses, best for complex tasks",
        ModelTier.GPT4_MINI: "Great balance of quality and speed, cost-effective",
        ModelTier.GPT35: "Fast responses, good for simple questions",
        ModelTier.LOCAL: "Private local model, no API costs, works offline"
    }
    return descriptions.get(tier, "AI language model")

@app.get("/api/models")
async def get_available_models():
    """Get list of all models for user selection (always shows all models)"""
    models = []
    for tier, config in MODEL_CONFIGS.items():
        try:
            # Check if model can be used for automatic selection
            is_auto_available = await model_manager._can_use_model(tier, 1000)
        except:
            is_auto_available = False
        
        # All models are always selectable manually, regardless of auto availability
        models.append({
            "id": tier.value,
            "name": config.name,
            "display_name": _get_model_display_name(tier, config),
            "description": _get_model_description(tier, config),
            "available": True,  # Always selectable for manual selection
            "auto_available": is_auto_available,  # Available for automatic selection
            "is_local": not config.requires_api,
            "max_tokens": config.max_tokens,
            "cost_per_1k": config.cost_per_1k_tokens,
            "failure_count": model_manager.failure_counts.get(tier, 0),
            "status": "ready" if is_auto_available else ("quota_exceeded" if model_manager.failure_counts.get(tier, 0) > 0 else "unavailable")
        })
    return {"models": models}

@app.post("/api/models/select")
async def select_model(request: dict):
    """Allow user to manually select a preferred model (overrides availability checks)"""
    model_id = request.get("model_id")
    if not model_id:
        raise HTTPException(status_code=400, detail="model_id is required")
    
    # Find the model tier
    selected_tier = None
    for tier in ModelTier:
        if tier.value == model_id:
            selected_tier = tier
            break
    
    if not selected_tier:
        raise HTTPException(status_code=400, detail=f"Invalid model_id: {model_id}")
    
    # Allow manual selection regardless of current availability status
    # Reset failure count for manually selected model to give it a fresh chance
    model_manager.failure_counts[selected_tier] = 0
    
    # Set the preferred model
    model_manager.user_preferred_model = selected_tier
    model_manager.current_tier = selected_tier
    model_manager.is_user_selected = True
    
    config = MODEL_CONFIGS[selected_tier]
    status_message = "Successfully switched to"
    
    # Check if model might have issues and warn user
    if not config.requires_api:
        status_message = "Successfully switched to"
    elif model_manager.client is None:
        status_message = "Switched to (Note: No OpenAI API key configured)"
    else:
        status_message = "Successfully switched to"
    
    return {
        "success": True,
        "message": f"{status_message} {_get_model_display_name(selected_tier, config)}",
        "current_model": model_manager.get_current_model_info()
    }

@app.post("/api/models/auto")
async def enable_auto_model():
    """Re-enable automatic model selection"""
    model_manager.user_preferred_model = None
    estimated_tokens = 1000
    selected_tier, reason = await model_manager.get_optimal_model(estimated_tokens)
    
    return {
        "success": True,
        "message": "Automatic model selection enabled",
        "current_model": model_manager.get_current_model_info(),
        "selection_reason": reason
    }

# Create upload directory
UPLOAD_DIR = Path("uploads")
UPLOAD_DIR.mkdir(exist_ok=True)

async def process_file_to_rag(file_path: Path, filename: str) -> tuple[int, str]:
    """
    å¤„ç†å•ä¸ªæ–‡ä»¶å¹¶æ·»åŠ åˆ°RAGç³»ç»Ÿ
    è¿”å› (chunk_count, processing_info)
    """
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.docstore.document import Document
    import uuid
    
    try:
        # 1. æ ¹æ®æ–‡ä»¶ç±»å‹æå–æ–‡æœ¬
        content = ""
        file_ext = file_path.suffix.lower()
        print(f"ğŸ” å¤„ç†æ–‡ä»¶: {filename}, æ‰©å±•å: {file_ext}")
        
        if file_ext == '.txt':
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        elif file_ext == '.md':
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        elif file_ext == '.py':
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                # ä¸ºä»£ç æ–‡ä»¶æ·»åŠ ä¸Šä¸‹æ–‡
                content = f"# Pythonä»£ç æ–‡ä»¶: {filename}\n\n{content}"
        elif file_ext in ['.pdf']:
            # PDFéœ€è¦é¢å¤–çš„åº“ï¼Œæš‚æ—¶è·³è¿‡
            return 0, f"PDFæ–‡ä»¶æš‚ä¸æ”¯æŒï¼Œè¯·è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼"
        elif file_ext in ['.docx']:
            # DOCXéœ€è¦é¢å¤–çš„åº“ï¼Œæš‚æ—¶è·³è¿‡  
            return 0, f"DOCXæ–‡ä»¶æš‚ä¸æ”¯æŒï¼Œè¯·è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼"
        elif file_ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp']:
            # å¤„ç†å›¾ç‰‡æ–‡ä»¶ - ä½¿ç”¨OpenAI Vision APIæå–æè¿°
            print(f"ğŸ–¼ï¸ æ£€æµ‹åˆ°å›¾ç‰‡æ–‡ä»¶ï¼Œå¼€å§‹AIè§†è§‰åˆ†æ: {filename}")
            try:
                import base64
                
                # è¯»å–å›¾ç‰‡å¹¶è½¬æ¢ä¸ºbase64
                with open(file_path, 'rb') as image_file:
                    image_content = image_file.read()
                    image_base64 = base64.b64encode(image_content).decode('utf-8')
                
                print(f"ğŸ“Š å›¾ç‰‡å¤§å°: {len(image_content)} bytes, base64é•¿åº¦: {len(image_base64)}")
                
                # ä½¿ç”¨OpenAI Vision APIåˆ†æå›¾ç‰‡
                print("ğŸ¤– è°ƒç”¨OpenAI Vision APIè¿›è¡Œå›¾ç‰‡åˆ†æ...")
                response = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": "Please provide a detailed description of this image, including any text, objects, people, scenes, or important details that would be useful for a knowledge base. If there's any text in the image, please transcribe it exactly."
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/{file_ext[1:]};base64,{image_base64}"
                                    }
                                }
                            ]
                        }
                    ],
                    max_tokens=1000
                )
                
                # æå–AIçš„æè¿°ä½œä¸ºæ–‡æ¡£å†…å®¹
                ai_description = response.choices[0].message.content
                print(f"âœ… AIè§†è§‰åˆ†æå®Œæˆï¼Œæè¿°é•¿åº¦: {len(ai_description)} å­—ç¬¦")
                
                content = f"# å›¾ç‰‡æ–‡ä»¶: {filename}\n\n## AIè§†è§‰åˆ†æç»“æœ:\n\n{ai_description}\n\n## æ–‡ä»¶ä¿¡æ¯:\n- æ–‡ä»¶å: {filename}\n- æ–‡ä»¶ç±»å‹: å›¾ç‰‡ ({file_ext})\n- åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
                
            except Exception as e:
                print(f"âŒ å›¾ç‰‡å¤„ç†å¤±è´¥: {e}")
                return 0, f"å›¾ç‰‡åˆ†æå¤±è´¥: {str(e)}"
        else:
            # å°è¯•ä½œä¸ºæ–‡æœ¬æ–‡ä»¶è¯»å–
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
            except:
                return 0, f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}"
        
        if not content.strip():
            return 0, "æ–‡ä»¶å†…å®¹ä¸ºç©º"
        
        # 2. åˆ†å‰²æ–‡æœ¬ä¸ºchunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        
        chunks = text_splitter.split_text(content)
        
        if not chunks:
            return 0, "æ–‡æœ¬åˆ†å‰²åä¸ºç©º"
        
        # 3. åˆ›å»ºæ–‡æ¡£å¯¹è±¡å¹¶æ·»åŠ å…ƒæ•°æ®
        documents = []
        for i, chunk in enumerate(chunks):
            doc = Document(
                page_content=chunk,
                metadata={
                    "source": filename,
                    "chunk_id": i,
                    "total_chunks": len(chunks),
                    "file_type": file_ext,
                    "upload_time": str(datetime.now()),
                    "doc_id": str(uuid.uuid4())
                }
            )
            documents.append(doc)
        
        # 4. æ·»åŠ åˆ°Pineconeå‘é‡æ•°æ®åº“
        try:
            if vectorstore is None:
                print(f"âš ï¸ å‘é‡æ•°æ®åº“ä¸å¯ç”¨ï¼Œè·³è¿‡å‘é‡åŒ–: {filename}")
                return len(documents), f"æ–‡æ¡£å·²å¤„ç†ä½†æœªå‘é‡åŒ–ï¼ˆå‘é‡æ•°æ®åº“ä¸å¯ç”¨ï¼‰"
            
            vectorstore.add_documents(documents)
            print(f"âœ… æˆåŠŸæ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£å—åˆ°å‘é‡æ•°æ®åº“: {filename}")
            return len(documents), f"æˆåŠŸå¤„ç†å¹¶å‘é‡åŒ– {len(documents)} ä¸ªæ–‡æœ¬å—"
        except Exception as e:
            print(f"âŒ å‘é‡æ•°æ®åº“æ·»åŠ å¤±è´¥: {e}")
            return len(documents), f"æ–‡æ¡£å·²å¤„ç†ä½†å‘é‡åŒ–å¤±è´¥: {str(e)}"
            
    except Exception as e:
        print(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥ {filename}: {e}")
        return 0, f"å¤„ç†å¤±è´¥: {str(e)}"

@app.post("/api/upload")
async def upload_files(files: List[UploadFile] = File(...)):
    """
    Handle file uploads for RAG learning
    å®Œæ•´å¤„ç†æ–‡ä»¶å¹¶æ·»åŠ åˆ°å‘é‡æ•°æ®åº“ï¼Œè®©AIèƒ½å¤Ÿå­¦ä¹ æ–°å†…å®¹
    """
    uploaded_filenames = []
    processing_results = []
    total_chunks = 0
    
    for file in files:
        # Save file to upload directory
        file_path = UPLOAD_DIR / file.filename
        
        try:
            with open(file_path, "wb") as f:
                content = await file.read()
                f.write(content)
            
            uploaded_filenames.append(file.filename)
            print(f"ğŸ“ æ–‡ä»¶å·²ä¿å­˜: {file.filename} ({len(content)} bytes)")
            
            # ğŸš€ å®Œæ•´çš„RAGå¤„ç†æµç¨‹
            chunk_count, process_info = await process_file_to_rag(file_path, file.filename)
            total_chunks += chunk_count
            
            processing_results.append({
                "filename": file.filename,
                "chunks_created": chunk_count,
                "status": "success" if chunk_count > 0 else "failed",
                "info": process_info
            })
            
            # ğŸ“ è®°å½•åˆ°AIè®°å¿†ç³»ç»Ÿ
            if chunk_count > 0:
                try:
                    memory_system.store_conversation(
                        session_id="system",
                        user_message=f"ç”¨æˆ·ä¸Šä¼ äº†æ–‡ä»¶: {file.filename}",
                        assistant_response=f"æˆåŠŸå­¦ä¹ äº†æ–°æ–‡æ¡£ã€Š{file.filename}ã€‹ï¼ŒåŒ…å« {chunk_count} ä¸ªçŸ¥è¯†ç‰‡æ®µã€‚ç°åœ¨æˆ‘å¯ä»¥åŸºäºè¿™ä¸ªæ–‡æ¡£å›ç­”ç›¸å…³é—®é¢˜äº†ã€‚",
                        context_used=f"æ–°å¢æ–‡æ¡£: {file.filename}",
                        topics=[file.filename, "æ–‡æ¡£å­¦ä¹ ", "çŸ¥è¯†æ›´æ–°"]
                    )
                except Exception as memory_error:
                    print(f"è®°å¿†ç³»ç»Ÿè®°å½•å¤±è´¥: {memory_error}")
            
        except Exception as e:
            processing_results.append({
                "filename": file.filename,
                "chunks_created": 0,
                "status": "failed",
                "info": f"ä¸Šä¼ å¤±è´¥: {str(e)}"
            })
    
    # ç”Ÿæˆè¯¦ç»†çš„å“åº”æ¶ˆæ¯
    success_count = sum(1 for r in processing_results if r["status"] == "success")
    message_parts = [
        f"ğŸ“¤ ä¸Šä¼ äº† {len(uploaded_filenames)} ä¸ªæ–‡ä»¶",
        f"âœ… æˆåŠŸå¤„ç† {success_count} ä¸ªæ–‡ä»¶",
        f"ğŸ§  æ€»è®¡å­¦ä¹ äº† {total_chunks} ä¸ªçŸ¥è¯†ç‰‡æ®µ"
    ]
    
    if total_chunks > 0:
        message_parts.append("ğŸ’¡ AIç°åœ¨å¯ä»¥åŸºäºè¿™äº›æ–°æ–‡æ¡£å›ç­”é—®é¢˜äº†ï¼")
    
    return {
        "status": "success" if success_count > 0 else "partial_failure",
        "files_count": len(uploaded_filenames),
        "filenames": uploaded_filenames,
        "processed_count": success_count,
        "total_chunks_created": total_chunks,
        "processing_results": processing_results,
        "message": " | ".join(message_parts)
    }

@app.post("/api/chat_with_image")
async def chat_with_image(
    session_id: str = Form(...),
    message: str = Form(""),
    max_history: int = Form(8),
    image: UploadFile = File(...)
):
    """Chat endpoint with image support using OpenAI Vision"""
    try:
        # Save the uploaded image
        image_path = UPLOAD_DIR / f"chat_{session_id}_{image.filename}"
        image_content = await image.read()
        
        with open(image_path, "wb") as f:
            f.write(image_content)
        
        print(f"ğŸ’¬ğŸ–¼ï¸ Image chat request - Session: {session_id}, Image: {image.filename}, Message: {message}")
        
        # Prepare messages for OpenAI Vision API
        messages = []
        
        # Add conversation history
        if session_id in SESSIONS:
            history = SESSIONS[session_id][-max_history:]
            for msg in history:
                messages.append({"role": msg["role"], "content": msg["content"]})
        
        # Create content for the current message
        content_parts = []
        if message.strip():
            content_parts.append({"type": "text", "text": message})
        
        # Add image
        import base64
        image_base64 = base64.b64encode(image_content).decode('utf-8')
        content_parts.append({
            "type": "image_url",
            "image_url": {
                "url": f"data:{image.content_type};base64,{image_base64}"
            }
        })
        
        messages.append({
            "role": "user", 
            "content": content_parts
        })
        
        # Call OpenAI Vision API
        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",  # Use vision-capable model
                messages=messages,
                max_tokens=1000
            )
            answer = response.choices[0].message.content
            
            # Store conversation
            memory_system.store_conversation(
                session_id, message, answer, 
                context_used=f"Image analysis: {image.filename}",
                topics=["image_analysis", "visual_content"]
            )
            
            SESSIONS.setdefault(session_id, []).extend([
                {"role": "user", "content": f"{message} [Image: {image.filename}]"},
                {"role": "assistant", "content": answer}
            ])
            
            return {"answer": answer}
            
        except Exception as e:
            print(f"âŒ OpenAI Vision API error: {e}")
            return {"answer": f"Sorry, I couldn't process the image. Error: {str(e)}"}
            
    except Exception as e:
        print(f"âŒ Image processing error: {e}")
        raise HTTPException(status_code=500, detail=f"Image processing failed: {str(e)}")

@app.post("/api/chat_stream_with_image")
async def chat_stream_with_image(
    session_id: str = Form(...),
    message: str = Form(""),
    max_history: int = Form(8),
    stream: str = Form("true"),
    image: UploadFile = File(...)
):
    """Streaming chat endpoint with image support using OpenAI Vision"""
    try:
        # Save the uploaded image
        image_path = UPLOAD_DIR / f"chat_stream_{session_id}_{image.filename}"
        image_content = await image.read()
        
        with open(image_path, "wb") as f:
            f.write(image_content)
        
        print(f"ğŸ’¬ğŸ–¼ï¸ Streaming image chat - Session: {session_id}, Image: {image.filename}, Message: {message}")
        
        def generate_stream():
            try:
                # Send model info
                yield _sse_format("model_info", '{"model": "gpt-4o-mini", "tier": "vision", "reason": "Image analysis mode"}')
                
                # Prepare messages for OpenAI Vision API
                messages = []
                
                # Add conversation history
                if session_id in SESSIONS:
                    history = SESSIONS[session_id][-max_history:]
                    for msg in history:
                        messages.append({"role": msg["role"], "content": msg["content"]})
                
                # Create content for the current message
                content_parts = []
                if message.strip():
                    content_parts.append({"type": "text", "text": message})
                
                # Add image
                import base64
                image_base64 = base64.b64encode(image_content).decode('utf-8')
                content_parts.append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:{image.content_type};base64,{image_base64}"
                    }
                })
                
                messages.append({
                    "role": "user", 
                    "content": content_parts
                })
                
                # Call OpenAI Vision API with streaming
                response = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=messages,
                    max_tokens=1000,
                    stream=True
                )
                
                full_answer = ""
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_answer += content
                        yield f"data: {content}"
                
                # Store conversation
                memory_system.store_conversation(
                    session_id, message, full_answer,
                    context_used=f"Image analysis: {image.filename}",
                    topics=["image_analysis", "visual_content"]
                )
                
                SESSIONS.setdefault(session_id, []).extend([
                    {"role": "user", "content": f"{message} [Image: {image.filename}]"},
                    {"role": "assistant", "content": full_answer}
                ])
                
                yield _sse_format("done", "")
                
            except Exception as e:
                print(f"âŒ Streaming image processing error: {e}")
                yield f"data: Sorry, I couldn't process the image. Error: {str(e)}"
                yield _sse_format("done", "")
        
        return StreamingResponse(
            generate_stream(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Headers": "*",
            }
        )
        
    except Exception as e:
        print(f"âŒ Streaming image setup error: {e}")
        raise HTTPException(status_code=500, detail=f"Streaming image setup failed: {str(e)}")

@app.post("/api/process_existing_files")
async def process_existing_files():
    """
    æ‰¹é‡å¤„ç†uploadsç›®å½•ä¸­å·²å­˜åœ¨ä½†æœªå¤„ç†çš„æ–‡ä»¶
    è®©AIèƒ½å¤Ÿå­¦ä¹ ä¹‹å‰ä¸Šä¼ ä½†æœªå‘é‡åŒ–çš„æ–‡ä»¶
    """
    if not UPLOAD_DIR.exists():
        return {"status": "no_files", "message": "uploadsç›®å½•ä¸å­˜åœ¨"}
    
    files = list(UPLOAD_DIR.glob("*"))
    files = [f for f in files if f.is_file()]
    
    if not files:
        return {"status": "no_files", "message": "uploadsç›®å½•ä¸­æ²¡æœ‰æ–‡ä»¶"}
    
    processing_results = []
    total_chunks = 0
    
    for file_path in files:
        print(f"ğŸ”„ å¤„ç†ç°æœ‰æ–‡ä»¶: {file_path.name}")
        chunk_count, process_info = await process_file_to_rag(file_path, file_path.name)
        total_chunks += chunk_count
        
        processing_results.append({
            "filename": file_path.name,
            "chunks_created": chunk_count,
            "status": "success" if chunk_count > 0 else "failed",
            "info": process_info
        })
        
        # è®°å½•åˆ°è®°å¿†ç³»ç»Ÿ
        if chunk_count > 0:
            try:
                memory_system.store_conversation(
                    session_id="system",
                    user_message=f"æ‰¹é‡å¤„ç†æ–‡ä»¶: {file_path.name}",
                    assistant_response=f"é‡æ–°å­¦ä¹ äº†æ–‡æ¡£ã€Š{file_path.name}ã€‹ï¼ŒåŒ…å« {chunk_count} ä¸ªçŸ¥è¯†ç‰‡æ®µã€‚",
                    context_used=f"æ‰¹é‡å¤„ç†: {file_path.name}",
                    topics=[file_path.name, "æ‰¹é‡å­¦ä¹ ", "çŸ¥è¯†æ›´æ–°"]
                )
            except Exception as memory_error:
                print(f"è®°å¿†ç³»ç»Ÿè®°å½•å¤±è´¥: {memory_error}")
    
    success_count = sum(1 for r in processing_results if r["status"] == "success")
    
    return {
        "status": "success" if success_count > 0 else "failed",
        "files_processed": len(files),
        "successful_files": success_count,
        "total_chunks_created": total_chunks,
        "processing_results": processing_results,
        "message": f"æ‰¹é‡å¤„ç†å®Œæˆï¼š{success_count}/{len(files)} ä¸ªæ–‡ä»¶æˆåŠŸï¼Œæ€»è®¡å­¦ä¹  {total_chunks} ä¸ªçŸ¥è¯†ç‰‡æ®µ"
    }

@app.get("/api/uploaded_files")
async def list_uploaded_files():
    """
    åˆ—å‡ºå·²ä¸Šä¼ çš„æ–‡ä»¶å’Œå®ƒä»¬çš„å¤„ç†çŠ¶æ€
    """
    if not UPLOAD_DIR.exists():
        return {"files": [], "message": "uploadsç›®å½•ä¸å­˜åœ¨"}
    
    files = []
    for file_path in UPLOAD_DIR.glob("*"):
        if file_path.is_file():
            stat = file_path.stat()
            files.append({
                "filename": file_path.name,
                "size": stat.st_size,
                "upload_time": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                "file_type": file_path.suffix.lower()
            })
    
    return {
        "files": files,
        "count": len(files),
        "message": f"Found {len(files)} uploaded files"
    }

# =====================================================
# ğŸ§  AIæˆé•¿ä¸å­¦ä¹ API (Memory & Personality System)
# =====================================================

class FeedbackRequest(BaseModel):
    conversation_id: str
    feedback_type: str  # 'positive', 'negative', 'correction', 'suggestion'
    content: str

class LearningStatsResponse(BaseModel):
    total_conversations: int
    learned_concepts: int
    total_feedback: int
    top_concepts: List[Dict]
    user_style_analysis: Dict

@app.post("/api/feedback")
async def add_feedback(feedback: FeedbackRequest):
    """ç”¨æˆ·åé¦ˆï¼Œè®©AIä»ä¸­å­¦ä¹ æ”¹è¿›"""
    try:
        memory_system.add_feedback(
            feedback.conversation_id,
            feedback.feedback_type,
            feedback.content
        )
        return {"status": "success", "message": "æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼æˆ‘ä¼šä»ä¸­å­¦ä¹ æ”¹è¿›ã€‚"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Feedback storage failed: {str(e)}")

@app.get("/api/learning_stats/{session_id}")
async def get_learning_stats(session_id: str):
    """è·å–AIçš„å­¦ä¹ ç»Ÿè®¡å’Œä¸ªæ€§åŒ–åˆ†æ"""
    try:
        # åŸºç¡€å­¦ä¹ ç»Ÿè®¡
        base_stats = memory_system.get_learning_stats()
        
        # ç”¨æˆ·é£æ ¼åˆ†æ
        user_style = personality_system.analyze_user_style(session_id)
        
        # è·å–è¯¥ç”¨æˆ·çš„å¯¹è¯å†å²ç»Ÿè®¡
        user_conversations = memory_system.get_relevant_memory("", session_id, limit=100)
        
        return LearningStatsResponse(
            total_conversations=base_stats['total_conversations'],
            learned_concepts=base_stats['learned_concepts'],
            total_feedback=base_stats['total_feedback'],
            top_concepts=base_stats['top_concepts'],
            user_style_analysis={
                **user_style,
                "user_conversations_count": len(user_conversations),
                "analysis": {
                    "communication_style": "Formal" if user_style['formality'] > 0.7 else "Casual" if user_style['formality'] < 0.3 else "Moderate",
                    "detail_preference": "Detailed" if user_style['detail_level'] > 0.7 else "Concise" if user_style['detail_level'] < 0.3 else "Moderate",
                    "friendliness_level": "Friendly" if user_style['friendliness'] > 0.5 else "Professional"
                }
            }
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Stats retrieval failed: {str(e)}")

@app.get("/api/memory_search/{session_id}")
async def search_memories(session_id: str, query: str = "", limit: int = 5):
    """æœç´¢ç›¸å…³çš„å¯¹è¯è®°å¿†"""
    try:
        memories = memory_system.get_relevant_memory(query, session_id, limit)
        return {
            "memories": memories,
            "count": len(memories),
            "query": query
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Memory search failed: {str(e)}")

@app.get("/api/growth_insights/{session_id}")
async def get_growth_insights(session_id: str):
    """è·å–AIæˆé•¿æ´å¯Ÿå’Œå»ºè®®"""
    try:
        stats = memory_system.get_learning_stats()
        user_style = personality_system.analyze_user_style(session_id)
        memories = memory_system.get_relevant_memory("", session_id, limit=50)
        
        # åˆ†ææˆé•¿è¶‹åŠ¿
        insights = {
            "learning_progress": {
                "conversations": stats['total_conversations'],
                "concepts_learned": stats['learned_concepts'],
                "feedback_received": stats['total_feedback'],
                "growth_rate": stats['learned_concepts'] / max(stats['total_conversations'], 1)
            },
            "personalization_level": {
                "style_adaptation": sum(user_style.values()) / len(user_style),
                "conversation_history": len(memories),
                "adaptation_quality": "é«˜" if len(memories) > 20 else "ä¸­" if len(memories) > 5 else "ä½"
            },
            "recommendations": []
        }
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        if stats['total_feedback'] < 5:
            insights["recommendations"].append("ğŸ’¬ å¤šç»™æˆ‘ä¸€äº›åé¦ˆï¼Œè¿™æ ·æˆ‘èƒ½æ›´å¥½åœ°äº†è§£æ‚¨çš„åå¥½")
        
        if stats['total_conversations'] > 10 and stats['learned_concepts'] < 5:
            insights["recommendations"].append("ğŸ¯ å°è¯•é—®æˆ‘ä¸€äº›æ›´å¤šæ ·çš„é—®é¢˜ï¼Œè®©æˆ‘å­¦ä¹ æ–°çš„æ¦‚å¿µ")
        
        if user_style['formality'] == 0.5:
            insights["recommendations"].append("ğŸ¨ ç»§ç»­ä¸æˆ‘å¯¹è¯ï¼Œæˆ‘æ­£åœ¨å­¦ä¹ é€‚åº”æ‚¨çš„äº¤æµé£æ ¼")
        
        return insights
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Growth insights failed: {str(e)}")