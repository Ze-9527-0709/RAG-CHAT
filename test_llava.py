#!/usr/bin/env python3
"""
LLaVA Integration Test Script
"""
import requests
import base64
import json

def test_llava_integration():
    """Test whether LLaVA model can process images"""
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•å›¾ç‰‡ï¼ˆbase64ç¼–ç çš„ç®€å•å›¾ç‰‡ï¼‰
    # è¿™æ˜¯ä¸€ä¸ª1x1åƒç´ çš„é€æ˜PNGå›¾ç‰‡
    test_image_base64 = "iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8/5+hHgAHggJ/PchI7wAAAABJRU5ErkJggg=="
    
    # è§£ç base64å›¾ç‰‡æ•°æ®
    import base64
    image_data = base64.b64decode(test_image_base64)
    
    try:
        print("ğŸ§ª Testing LLaVA image processing functionality...")
        
        # å‡†å¤‡è¡¨å•æ•°æ®å’Œæ–‡ä»¶
        form_data = {
            'session_id': 'test_session',
            'message': 'Please describe this image',
            'max_history': '8'
        }
        
        files = {
            'image': ('test.png', image_data, 'image/png')
        }
        
        # å‘é€è¯·æ±‚åˆ°åç«¯
        response = requests.post(
            "http://localhost:8000/api/chat_with_image", 
            data=form_data,
            files=files,
            timeout=30
        )
        
        print(f"ğŸ“¡ HTTP status code: {response.status_code}")
        
        if response.status_code == 200:
            result = response.json()
            print(f"âœ… LLaVA response successful!")
            print(f"ğŸ“ Response content: {result.get('response', 'No response')[:200]}...")
            return True
        else:
            print(f"âŒ Request failed: {response.text}")
            return False
            
    except requests.exceptions.RequestException as e:
        print(f"âŒ Network error: {e}")
        return False
    except Exception as e:
        print(f"âŒ Other error: {e}")
        return False

def test_ollama_direct():
    """Direct test of Ollama LLaVA model"""
    try:
        print("\nğŸ” Direct testing Ollama LLaVA...")
        
        test_data = {
            "model": "llava:7b",
            "prompt": "Describe this image",
            "images": ["iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8/5+hHgAHggJ/PchI7wAAAABJRU5ErkJggg=="],
            "stream": False
        }
        
        response = requests.post(
            "http://localhost:11434/api/generate",
            json=test_data,
            timeout=30
        )
        
        print(f"ğŸ“¡ Ollama HTTP status code: {response.status_code}")
        
        if response.status_code == 200:
            result = response.json()
            print(f"âœ… Ollama LLaVA response successful!")
            print(f"ğŸ“ Response content: {result.get('response', 'No response')[:200]}...")
            return True
        else:
            print(f"âŒ Ollama request failed: {response.text}")
            return False
            
    except Exception as e:
        print(f"âŒ Ollama test error: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ Starting LLaVA integration test...")
    
    # é¦–å…ˆæµ‹è¯•ç›´æ¥Ollamaè¿æ¥
    ollama_ok = test_ollama_direct()
    
    # ç„¶åæµ‹è¯•é€šè¿‡æˆ‘ä»¬çš„åç«¯
    backend_ok = test_llava_integration()
    
    print("\nğŸ“Š Test results:")
    print(f"   Ollama Direct: {'âœ… Success' if ollama_ok else 'âŒ Failed'}")
    print(f"   Backend Integration: {'âœ… Success' if backend_ok else 'âŒ Failed'}")
    
    if ollama_ok and backend_ok:
        print("\nğŸ‰ LLaVA integration test completely successful!")
    elif ollama_ok:
        print("\nâš ï¸ Ollama works normally, but backend integration has issues")
    else:
        print("\nâŒ LLaVA integration has issues, needs checking")